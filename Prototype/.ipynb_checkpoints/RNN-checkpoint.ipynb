{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import textgenrnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Datasets/Reddit/post_comments.pickle\"\n",
    "import pickle\n",
    "reddit_data = pickle.load(open(path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for k in reddit_data:\n",
    "    sub = reddit_data[k]\n",
    "    cmnts = [p[1] for p in sub]\n",
    "    comments += cmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../Datasets/Reddit/comments.txt\", \"w\")\n",
    "[f.write(comment + \"\\n\") for comment in comments]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = {\n",
    "    'rnn_size': 128,\n",
    "    'rnn_layers': 12,\n",
    "    'rnn_bidirectional': True,\n",
    "    'max_length': 15,\n",
    "    'max_words': 10000,\n",
    "    'dim_embeddings': 100,\n",
    "    'word_level': True,\n",
    "}\n",
    "train_cfg = {\n",
    "    'line_delimited': True,\n",
    "    'num_epochs': 2,\n",
    "    'gen_epochs': 1,\n",
    "    'batch_size': 25,\n",
    "    'train_size': 1.0,\n",
    "    'dropout': 0.0,\n",
    "    'max_gen_length': 150,\n",
    "    'validation': False,\n",
    "    'is_csv': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 texts collected.\n",
      "Training on 80,122 character sequences.\n",
      "Epoch 1/2\n",
      "3204/3204 [==============================] - 269s 84ms/step - loss: 1.7706\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I have the stre so the strange the stres with the strange and the strange I can be a life of the way the strange I want to the strange and the was a \n",
      "\n",
      "I have the stress and the want of the stress the strem the stress and I can all how I can and the strange what I can all the strae the strange of the\n",
      "\n",
      "I want you the was a life when you have the same of your stress. I want to the strang that a life of the show when a stre want the world who want to \n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I fell lot found up what a bude and depression in a strange the hours and he was this want a strest weal. I can't he was had when you street and the \n",
      "\n",
      "I don't have out of the god while happing when a shopper witch the applyone still the continue of you and all my same that I can't chot a while when \n",
      "\n",
      "Yeg don't that had a chot the fucking and when you don’t have for the same the thought still when well. The prette thinks abulated that I’ll be checa\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I grow a most depression mething time encournte. I was fucks, you're killy a in shows to live more, it looks help actually just the placr caseeless.\n",
      "\n",
      "I hand you.\n",
      "\n",
      "When your Only I can't want a more wher than gett like it quessess soon.\n",
      "\n",
      "Epoch 2/2\n",
      "3204/3204 [==============================] - 267s 83ms/step - loss: 1.4752\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I hope you feel like the struggy and I don’t want to go about my life and the see I can get a competical like a lot of the world coups the second of \n",
      "\n",
      "I was the couple things and someone who was gotten the stranger and someone who with the second of the mother whenever I can be a completely thing th\n",
      "\n",
      "I can be a comment and the people when I can get me so much the world things and I can be a come thing the second of the moment than the couple thing\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "And my house and worked that been like me and don’t want to do that being a something so I can but the seems nothing things to mean you so I want to \n",
      "\n",
      "I feel like the spent is someone that so I can be commenting to mean home of my should whenever we can one of the most assion and I can don't see the\n",
      "\n",
      "As a so much the couple things from a thing and help me and the spent things I got last person who least the couple is a cut comfort the couple thing\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Good my like up god like down\n",
      "\n",
      "Will girl all gold and my Jubido! It's a sleaw, Also my spendes, cute this possible, and you great how wath woman eschool coming concerfeaming and ba\n",
      "\n",
      "I’m this exare just so who no extressed to be kill worrying eation want you!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "model_name = 'test_reddit'\n",
    "textgen = textgenrnn(name=model_name)\n",
    "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
    "train_function(\n",
    "    file_path=\"../Datasets/Reddit/comments.txt\",\n",
    "#     new_model=True,\n",
    "    num_epochs=train_cfg['num_epochs'],\n",
    "    gen_epochs=train_cfg['gen_epochs'],\n",
    "    batch_size=train_cfg['batch_size'],\n",
    "    train_size=train_cfg['train_size'],\n",
    "    dropout=train_cfg['dropout'],\n",
    "    max_gen_length=train_cfg['max_gen_length'],\n",
    "    validation=train_cfg['validation'],\n",
    "    is_csv=train_cfg['is_csv'],\n",
    "    rnn_layers=model_cfg['rnn_layers'],\n",
    "    rnn_size=model_cfg['rnn_size'],\n",
    "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
    "    max_length=model_cfg['max_length'],\n",
    "    dim_embeddings=model_cfg['dim_embeddings'],\n",
    "    word_level=model_cfg['word_level']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
